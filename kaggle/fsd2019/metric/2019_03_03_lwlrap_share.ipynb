{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019-03-03_lwlrap-share.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq8VVHsTohAy",
        "colab_type": "text"
      },
      "source": [
        "# 2019-03-03_lwlrap-share\n",
        "\n",
        "Reference implementation of l$\\omega$lrap both natively and using sklearn.metrics.\n",
        "\n",
        "Dan Ellis dpwe@google.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHIGP6A9ogyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xffu7w5t0YFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Core calculation of label precisions for one test sample.\n",
        "\n",
        "def _one_sample_positive_class_precisions(scores, truth):\n",
        "  \"\"\"Calculate precisions for each true class for a single sample.\n",
        "  \n",
        "  Args:\n",
        "    scores: np.array of (num_classes,) giving the individual classifier scores.\n",
        "    truth: np.array of (num_classes,) bools indicating which classes are true.\n",
        "\n",
        "  Returns:\n",
        "    pos_class_indices: np.array of indices of the true classes for this sample.\n",
        "    pos_class_precisions: np.array of precisions corresponding to each of those\n",
        "      classes.\n",
        "  \"\"\"\n",
        "  num_classes = scores.shape[0]\n",
        "  pos_class_indices = np.flatnonzero(truth > 0)\n",
        "  # Only calculate precisions if there are some true classes.\n",
        "  if not len(pos_class_indices):\n",
        "    return pos_class_indices, np.zeros(0)\n",
        "  # Retrieval list of classes for this sample. \n",
        "  retrieved_classes = np.argsort(scores)[::-1]\n",
        "  # class_rankings[top_scoring_class_index] == 0 etc.\n",
        "  class_rankings = np.zeros(num_classes, dtype=np.int)\n",
        "  class_rankings[retrieved_classes] = range(num_classes)\n",
        "  # Which of these is a true label?\n",
        "  retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
        "  retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
        "  # Num hits for every truncated retrieval list.\n",
        "  retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
        "  # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
        "  precision_at_hits = (\n",
        "      retrieved_cumulative_hits[class_rankings[pos_class_indices]] / \n",
        "      (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
        "  return pos_class_indices, precision_at_hits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HfziEYbodWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All-in-one calculation of per-class lwlrap.\n",
        "\n",
        "def calculate_per_class_lwlrap(truth, scores):\n",
        "  \"\"\"Calculate label-weighted label-ranking average precision.\n",
        "  \n",
        "  Arguments:\n",
        "    truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
        "      of presence of that class in that sample.\n",
        "    scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
        "      test's real-valued score for each class for each sample.\n",
        "  \n",
        "  Returns:\n",
        "    per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each \n",
        "      class.\n",
        "    weight_per_class: np.array of (num_classes,) giving the prior of each \n",
        "      class within the truth labels.  Then the overall unbalanced lwlrap is \n",
        "      simply np.sum(per_class_lwlrap * weight_per_class)\n",
        "  \"\"\"\n",
        "  assert truth.shape == scores.shape\n",
        "  num_samples, num_classes = scores.shape\n",
        "  # Space to store a distinct precision value for each class on each sample.\n",
        "  # Only the classes that are true for each sample will be filled in.\n",
        "  precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
        "  for sample_num in range(num_samples):\n",
        "    pos_class_indices, precision_at_hits = (\n",
        "      _one_sample_positive_class_precisions(scores[sample_num, :], \n",
        "                                            truth[sample_num, :]))\n",
        "    precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
        "        precision_at_hits)\n",
        "  labels_per_class = np.sum(truth > 0, axis=0)\n",
        "  weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
        "  # Form average of each column, i.e. all the precisions assigned to labels in\n",
        "  # a particular class.\n",
        "  per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) / \n",
        "                      np.maximum(1, labels_per_class))\n",
        "  # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
        "  #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
        "  #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
        "  #                = np.sum(per_class_lwlrap * weight_per_class)\n",
        "  return per_class_lwlrap, weight_per_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52LPXQNPppex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the overall lwlrap using sklearn.metrics function.\n",
        "\n",
        "def calculate_overall_lwlrap_sklearn(truth, scores):\n",
        "  \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n",
        "  # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n",
        "  sample_weight = np.sum(truth > 0, axis=1)\n",
        "  nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
        "  overall_lwlrap = sklearn.metrics.label_ranking_average_precision_score(\n",
        "      truth[nonzero_weight_sample_indices, :] > 0, \n",
        "      scores[nonzero_weight_sample_indices, :], \n",
        "      sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
        "  return overall_lwlrap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJv0Rtqfsu3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accumulator object version.\n",
        "\n",
        "class lwlrap_accumulator(object):\n",
        "  \"\"\"Accumulate batches of test samples into per-class and overall lwlrap.\"\"\"  \n",
        "\n",
        "  def __init__(self):\n",
        "    self.num_classes = 0\n",
        "    self.total_num_samples = 0\n",
        "  \n",
        "  def accumulate_samples(self, batch_truth, batch_scores):\n",
        "    \"\"\"Cumulate a new batch of samples into the metric.\n",
        "    \n",
        "    Args:\n",
        "      truth: np.array of (num_samples, num_classes) giving boolean\n",
        "        ground-truth of presence of that class in that sample for this batch.\n",
        "      scores: np.array of (num_samples, num_classes) giving the \n",
        "        classifier-under-test's real-valued score for each class for each\n",
        "        sample.\n",
        "    \"\"\"\n",
        "    assert batch_scores.shape == batch_truth.shape\n",
        "    num_samples, num_classes = batch_truth.shape\n",
        "    if not self.num_classes:\n",
        "      self.num_classes = num_classes\n",
        "      self._per_class_cumulative_precision = np.zeros(self.num_classes)\n",
        "      self._per_class_cumulative_count = np.zeros(self.num_classes, \n",
        "                                                  dtype=np.int)\n",
        "    assert num_classes == self.num_classes\n",
        "    for truth, scores in zip(batch_truth, batch_scores):\n",
        "      pos_class_indices, precision_at_hits = (\n",
        "        _one_sample_positive_class_precisions(scores, truth))\n",
        "      self._per_class_cumulative_precision[pos_class_indices] += (\n",
        "        precision_at_hits)\n",
        "      self._per_class_cumulative_count[pos_class_indices] += 1\n",
        "    self.total_num_samples += num_samples\n",
        "\n",
        "  def per_class_lwlrap(self):\n",
        "    \"\"\"Return a vector of the per-class lwlraps for the accumulated samples.\"\"\"\n",
        "    return (self._per_class_cumulative_precision / \n",
        "            np.maximum(1, self._per_class_cumulative_count))\n",
        "\n",
        "  def per_class_weight(self):\n",
        "    \"\"\"Return a normalized weight vector for the contributions of each class.\"\"\"\n",
        "    return (self._per_class_cumulative_count / \n",
        "            float(np.sum(self._per_class_cumulative_count)))\n",
        "\n",
        "  def overall_lwlrap(self):\n",
        "    \"\"\"Return the scalar overall lwlrap for cumulated samples.\"\"\"\n",
        "    return np.sum(self.per_class_lwlrap() * self.per_class_weight())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRCaCIb9oguU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random test data.\n",
        "num_samples = 100\n",
        "num_labels = 20\n",
        "\n",
        "truth = np.random.rand(num_samples, num_labels) > 0.5\n",
        "# Ensure at least some samples with no truth labels.\n",
        "truth[0:1, :] = False\n",
        "\n",
        "scores = np.random.rand(num_samples, num_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkXJOnMiqQVa",
        "colab_type": "code",
        "outputId": "6cc526f9-37cd-4cc2-ce9c-613375de37a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "per_class_lwlrap, weight_per_class = calculate_per_class_lwlrap(truth, scores)\n",
        "print(\"lwlrap from per-class values=\", np.sum(per_class_lwlrap * weight_per_class))\n",
        "print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lwlrap from per-class values= 0.5819169218307428\n",
            "lwlrap from sklearn.metrics = 0.581916921830743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVtzP2u-0FvO",
        "colab_type": "code",
        "outputId": "bd7a2515-484e-40e2-a0b6-a7e88f5c3c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Test of accumulator version.\n",
        "accumulator = lwlrap_accumulator()\n",
        "batch_size = 12\n",
        "for base_sample in range(0, scores.shape[0], batch_size):\n",
        "  accumulator.accumulate_samples(\n",
        "      truth[base_sample : base_sample + batch_size, :], \n",
        "      scores[base_sample : base_sample + batch_size, :])\n",
        "print(\"cumulative_lwlrap=\", accumulator.overall_lwlrap())\n",
        "print(\"total_num_samples=\", accumulator.total_num_samples)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cumulative_lwlrap= 0.5819169218307428\n",
            "total_num_samples= 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJhxzy7_0PRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}