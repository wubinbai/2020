%!PS-Adobe-3.0
%%Title: smaller_faster.txt
%%For: wb
%%Creator: VIM - Vi IMproved 8.0 (2016 Sep 12)
%%CreationDate: Tue Feb 25 20:55:40 2020
%%DocumentData: Clean8Bit
%%Orientation: Portrait
%%Pages: (atend)
%%PageOrder: Ascend
%%BoundingBox: 59 45 559 800
%%DocumentMedia: A4 595 842 0 () ()
%%DocumentNeededResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%DocumentSuppliedResources: procset VIM-Prolog 1.4 1
%%+ encoding VIM-latin1 1.0 0
%%Requirements: duplex collate
%%EndComments
%%BeginDefaults
%%PageResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%PageMedia: A4
%%EndDefaults
%%BeginProlog
%%BeginResource: procset VIM-Prolog
%%BeginDocument: /usr/share/vim/vim80/print/prolog.ps
%!PS-Adobe-3.0 Resource-ProcSet
%%Title: VIM-Prolog
%%Version: 1.4 1
%%EndComments
% Editing of this file is NOT RECOMMENDED.  You run a very good risk of causing
% all PostScript printing from VIM failing if you do.  PostScript is not called
% a write-only language for nothing!
/packedarray where not{userdict begin/setpacking/pop load def/currentpacking
false def end}{pop}ifelse/CP currentpacking def true setpacking
/bd{bind def}bind def/ld{load def}bd/ed{exch def}bd/d/def ld
/db{dict begin}bd/cde{currentdict end}bd
/T true d/F false d
/SO null d/sv{/SO save d}bd/re{SO restore}bd
/L2 systemdict/languagelevel 2 copy known{get exec}{pop pop 1}ifelse 2 ge d
/m/moveto ld/s/show ld /ms{m s}bd /g/setgray ld/r/setrgbcolor ld/sp{showpage}bd
/gs/gsave ld/gr/grestore ld/cp/currentpoint ld
/ul{gs UW setlinewidth cp UO add 2 copy newpath m 3 1 roll add exch lineto
stroke gr}bd
/bg{gs r cp BO add 4 -2 roll rectfill gr}bd
/sl{90 rotate 0 exch translate}bd
L2{
/sspd{mark exch{setpagedevice}stopped cleartomark}bd
/nc{1 db/NumCopies ed cde sspd}bd
/sps{3 db/Orientation ed[3 1 roll]/PageSize ed/ImagingBBox null d cde sspd}bd
/dt{2 db/Tumble ed/Duplex ed cde sspd}bd
/c{1 db/Collate ed cde sspd}bd
}{
/nc{/#copies ed}bd
/sps{statusdict/setpage get exec}bd
/dt{statusdict/settumble 2 copy known{get exec}{pop pop pop}ifelse
statusdict/setduplexmode 2 copy known{get exec}{pop pop pop}ifelse}bd
/c{pop}bd
}ifelse
/ffs{findfont exch scalefont d}bd/sf{setfont}bd
/ref{1 db findfont dup maxlength dict/NFD ed{exch dup/FID ne{exch NFD 3 1 roll
put}{pop pop}ifelse}forall/Encoding findresource dup length 256 eq{NFD/Encoding
3 -1 roll put}{pop}ifelse NFD dup/FontType get 3 ne{/CharStrings}{/CharProcs}
ifelse 2 copy known{2 copy get dup maxlength dict copy[/questiondown/space]{2
copy known{2 copy get 2 index/.notdef 3 -1 roll put pop exit}if pop}forall put
}{pop pop}ifelse dup NFD/FontName 3 -1 roll put NFD definefont pop end}bd
CP setpacking
(\004)cvn{}bd
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%BeginResource: encoding VIM-latin1
%%BeginDocument: /usr/share/vim/vim80/print/latin1.ps
%!PS-Adobe-3.0 Resource-Encoding
%%Title: VIM-latin1
%%Version: 1.0 0
%%EndComments
/VIM-latin1[
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclam /quotedbl /numbersign /dollar /percent /ampersand /quotesingle
/parenleft /parenright /asterisk /plus /comma /minus /period /slash
/zero /one /two /three /four /five /six /seven
/eight /nine /colon /semicolon /less /equal /greater /question
/at /A /B /C /D /E /F /G
/H /I /J /K /L /M /N /O
/P /Q /R /S /T /U /V /W
/X /Y /Z /bracketleft /backslash /bracketright /asciicircum /underscore
/grave /a /b /c /d /e /f /g
/h /i /j /k /l /m /n /o
/p /q /r /s /t /u /v /w
/x /y /z /braceleft /bar /braceright /asciitilde /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclamdown /cent /sterling /currency /yen /brokenbar /section
/dieresis /copyright /ordfeminine /guillemotleft /logicalnot /hyphen /registered /macron
/degree /plusminus /twosuperior /threesuperior /acute /mu /paragraph /periodcentered
/cedilla /onesuperior /ordmasculine /guillemotright /onequarter /onehalf /threequarters /questiondown
/Agrave /Aacute /Acircumflex /Atilde /Adieresis /Aring /AE /Ccedilla
/Egrave /Eacute /Ecircumflex /Edieresis /Igrave /Iacute /Icircumflex /Idieresis
/Eth /Ntilde /Ograve /Oacute /Ocircumflex /Otilde /Odieresis /multiply
/Oslash /Ugrave /Uacute /Ucircumflex /Udieresis /Yacute /Thorn /germandbls
/agrave /aacute /acircumflex /atilde /adieresis /aring /ae /ccedilla
/egrave /eacute /ecircumflex /edieresis /igrave /iacute /icircumflex /idieresis
/eth /ntilde /ograve /oacute /ocircumflex /otilde /odieresis /divide
/oslash /ugrave /uacute /ucircumflex /udieresis /yacute /thorn /ydieresis]
/Encoding defineresource pop
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%EndProlog
%%BeginSetup
595 842 0 sps
1 nc
T F dt
T c
%%IncludeResource: font Courier
/_F0 /VIM-latin1 /Courier ref
/F0 13 /_F0 ffs
%%IncludeResource: font Courier-Bold
/_F1 /VIM-latin1 /Courier-Bold ref
/F1 13 /_F1 ffs
%%IncludeResource: font Courier-Oblique
/_F2 /VIM-latin1 /Courier-Oblique ref
/F2 13 /_F2 ffs
%%IncludeResource: font Courier-BoldOblique
/_F3 /VIM-latin1 /Courier-BoldOblique ref
/F3 13 /_F3 ffs
/UO -1.3 d
/UW 0.65 d
/BO -3.25 d
%%EndSetup
%%Page: 1 1
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(smaller_faster.txt                                        Page 1)59.5 790.15 ms
F0 sf
(9th place solution: smaller and faster)59.5 764.15 ms
(by Iafossin Freesound Audio Tagging 2019 8 months ago)59.5 751.15 ms
(First, I would like to congratulate all winers and participants )59.5 725.15 ms
(of this competition and thank kaggle and the organizers for post)59.5 712.15 ms
(ing this interesting challenge and providing resources for parti)59.5 699.15 ms
(cipating in it. Also, I want to express my highest gratitude and)59.5 686.15 ms
( appreciation to my teammates: @suicaokhoailang, @theoviel, and )59.5 673.15 ms
(@yanglehan for working with me on this challenge. It was the fir)59.5 660.15 ms
(st time when I worked with audio related task, and it was a mont)59.5 647.15 ms
(h of quite intense learning. I would like to give special thanks)59.5 634.15 ms
( to @theoviel for working quite hard with me on optimization of )59.5 621.15 ms
(our models and prepare the final submissions.)59.5 608.15 ms
(The code used for training our final models is available now at )59.5 582.15 ms
(this link \(private LB 0.72171 score, top 42, with a single model)59.5 569.15 ms
( \(5 CV folds\) trained for 7 hours\). Further details of our solut)59.5 556.15 ms
(ion can be found in our DCASE2019 technical report.)59.5 543.15 ms
(The key points of our solutions are the following \(see details b)59.5 517.15 ms
(elow\): \(1\) Use of small and fast models with optimized architect)59.5 504.15 ms
(ure, \(2\) Use of 64 mels instead of 128 \(sometimes less gives mor)59.5 491.15 ms
(e\) with 4s duration, \(3\) Use data augmentation and noisy data fo)59.5 478.15 ms
(r pretraining.)59.5 465.15 ms
(Stage 1: we started the competition, like many participants, wit)59.5 439.15 ms
(h experimenting with common computer vision models. The input si)59.5 426.15 ms
(ze of spectrograms was 256x512 pixels, with upscaling the input )59.5 413.15 ms
(image along first dimension by a factor of 2. With this setup th)59.5 400.15 ms
(e best performance was demonstrated by DenseNet models: they out)59.5 387.15 ms
(performed the baseline model published in this kernel and succes)59.5 374.15 ms
(sfully used in the competition a year ago, also Dnet121 was fast)59.5 361.15 ms
(er. With using pretraining on full noisy set, spectral augmentat)59.5 348.15 ms
(ion, and MixUp, CV could reach ~0.85+, and public score for a si)59.5 335.15 ms
(ngle fold, 4 folds, and ensemble of 4 models are 0.67-0.68, ~0.7)59.5 322.15 ms
(0, 0.717, respectively. Despite these models are not used for ou)59.5 309.15 ms
(r submission, these experiments have provided important insights)59.5 296.15 ms
( for the next stage.)59.5 283.15 ms
(Stage 2:)59.5 257.15 ms
(Use of noisy data: It is the main point of this competition that)59.5 244.15 ms
( organizers wanted us to focus on \(no external data, no pretrain)59.5 231.15 ms
(ed models, no test data use policies\). We have used 2 strategies)59.5 218.15 ms
(: \(1\) pretraining on full noisy data and \(2\) pretraining on a mi)59.5 205.15 ms
(xture of the curated data with most confidently labeled noisy da)59.5 192.15 ms
(ta. In both cases the pretraining is followed by fine tuning on )59.5 179.15 ms
(curated only data. The most confident labels are identified base)59.5 166.15 ms
(d on a model trained on curated data, and further details can be)59.5 153.15 ms
( provided by @theoviel. For our best setup we have the following)59.5 140.15 ms
( values of CV \(in stage 2 we use 5 fold scheme\): training on cur)59.5 127.15 ms
(ated data only - 0.858, pretraining on full noisy data - 0.866, )59.5 114.15 ms
(curated data + 15k best noisy data examples - 0.865, curated dat)59.5 101.15 ms
(a + 5k best noisy data examples - 0.872. We have utilized all 3 )59.5 88.15 ms
(strategies of noisy data use to create a variety in our ensemble)59.5 75.15 ms
(.)59.5 62.15 ms
re sp
%%PageTrailer
%%Page: 2 2
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(smaller_faster.txt                                        Page 2)59.5 790.15 ms
F0 sf
(Preprocessing: According to our experiments, big models, like Dn)59.5 764.15 ms
(et121, work better on 128 mels and even higher image resolution,)59.5 751.15 ms
( while the default model reaches the best performance for 64 mel)59.5 738.15 ms
(s. This setup also decreases training time and improves converge)59.5 725.15 ms
(nce speed. 32 mels also could be considered, but the performence)59.5 712.15 ms
( drops to 0.856 for our best setup. Use of 4s intervals instead )59.5 699.15 ms
(of traditional 2s has gave also a considerable boost. The input )59.5 686.15 ms
(image size for the model is 64x256x1. We tried both normalizatio)59.5 673.15 ms
(n of data based on image and global train set statistics, and th)59.5 660.15 ms
(e results were similar. Though, our best CV is reached for globa)59.5 647.15 ms
(l normalization. In final models we used both strategies to crea)59.5 634.15 ms
(se a diversity. We also tried to experiment with the fft window )59.5 621.15 ms
(size but did not see a significant difference and stayed with 19)59.5 608.15 ms
(20. One thing to try we didn't have time for is using different )59.5 595.15 ms
(window sizes mels as channels of the produced image. In particul)59.5 582.15 ms
(ar, this paper shows that some classes prefer longer while other)59.5 569.15 ms
( shorter window size. The preprocessing pipeline is similar to o)59.5 556.15 ms
(ne described in this kernel.)59.5 543.15 ms
(Model architecture: At stage 2 we used the model from this kerne)59.5 517.15 ms
(l as a starting point. The performance of this base model for ou)59.5 504.15 ms
(r best setup is 0.855 CV. Based on our prior positive experience)59.5 491.15 ms
( with DensNet, we added dense connections inside convolution blo)59.5 478.15 ms
(cks and concatenate pooling that boosted the performance to 0.86)59.5 465.15 ms
(8 in our best experiments \(model M1\):)59.5 452.15 ms
(class ConvBlock\(nn.Module\):)59.5 426.15 ms
(    def __init__\(self, in_channels, out_channels, kernel_size=3,)59.5 413.15 ms
( pool=True\):)59.5 400.15 ms
(        super\(\).__init__\(\))59.5 387.15 ms
(        padding = kernel_size // 2)59.5 361.15 ms
(        self.pool = pool)59.5 348.15 ms
(        self.conv1 = nn.Sequential\()59.5 322.15 ms
(            nn.Conv2d\(in_channels, out_channels, kernel_size=ker)59.5 309.15 ms
(nel_size,)59.5 296.15 ms
(                stride=1, padding=padding\),)59.5 283.15 ms
(            nn.BatchNorm2d\(out_channels\),)59.5 270.15 ms
(            nn.ReLU\(\),)59.5 257.15 ms
(        \))59.5 244.15 ms
(        self.conv2 = nn.Sequential\()59.5 231.15 ms
(            nn.Conv2d\(out_channels + in_channels, out_channels, )59.5 218.15 ms
(                kernel_size=kernel_size, stride=1, padding=paddi)59.5 205.15 ms
(ng\),)59.5 192.15 ms
(            nn.BatchNorm2d\(out_channels\),)59.5 179.15 ms
(            nn.ReLU\(\),)59.5 166.15 ms
(        \))59.5 153.15 ms
(    def forward\(self, x\): # x.shape = [batch_size, in_channels, )59.5 127.15 ms
(a, b])59.5 114.15 ms
(        x1 = self.conv1\(x\))59.5 101.15 ms
(        x = self.conv2\(torch.cat\([x, x1],1\)\))59.5 88.15 ms
(        if\(self.pool\): x = F.avg_pool2d\(x, 2\))59.5 75.15 ms
(        return x   # x.shape = [batch_size, out_channels, a//2, )59.5 62.15 ms
(b//2])59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 3 3
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(smaller_faster.txt                                        Page 3)59.5 790.15 ms
F0 sf
(The increase of the number of convolution blocks from 4 to 5 gav)59.5 751.15 ms
(e only 0.865 CV. Use of a pyramidal pooling for 2,3 and 4-th con)59.5 738.15 ms
(v blocks \(M2\) gave slightly worse result than M1. Finally, our u)59.5 725.15 ms
(ltimate setup \(M3\) consists of 5 conv blocks with pyramidal pool)59.5 712.15 ms
(ing reached 0.872 CV. DenseNet121 in the same pipeline reached o)59.5 699.15 ms
(nly 0.836 \(DenseNet121 requires higher image resolution, 256x512)59.5 686.15 ms
(, to reach 0.85+ CV\). From the experiments, it looks for audio i)59.5 673.15 ms
(t is important to have nonlinear operations before size reductio)59.5 660.15 ms
(n by pooling, though we did not check it in details. We used M1,)59.5 647.15 ms
( M2, and M3 to create variability in our ensemble. Because of th)59.5 634.15 ms
(e submission limit we checked performance of only a few models i)59.5 621.15 ms
(n public LB, with the best single model score 0.715.)59.5 608.15 ms
(This plot illustrates architecture of M3 model:)59.5 582.15 ms
(Here, all tested models are summarized:)59.5 556.15 ms
(And here the model performance for our best setup:)59.5 530.15 ms
(Data augmentation:)59.5 504.15 ms
(The main thing for working with audio data is MixUp. In contrast)59.5 491.15 ms
( to images of real objects, sounds are transparent: they do not )59.5 478.15 ms
(overshadow each other. Therefore, MixUp is so efficient for audi)59.5 465.15 ms
(o and gives 0.01-0.015 CV boost. At the stage 1 the best results)59.5 452.15 ms
( were achieved for alpha MixUp parameter equal to 1.0, while at )59.5 439.15 ms
(the stage 2 we used 0.4. Spectral augmentation \(with 2 masks for)59.5 426.15 ms
( frequency and time with the coverage range between zero and 0.1)59.5 413.15 ms
(5 and 0.3, respectively\) gave about 0.005 CV boost. We did not u)59.5 400.15 ms
(se stretching the spectrograms in time domain because it gave lo)59.5 387.15 ms
(wer model performance. In several model we also used Multisample)59.5 374.15 ms
( Dropout \(other models were trained without dropout\); though, it)59.5 361.15 ms
( decreased CV by ~0.002. We did not apply horizontal flip since )59.5 348.15 ms
(it decreased CV and also is not natural: I do not think that peo)59.5 335.15 ms
(ple would be able to recognize sounds played from the back. It i)59.5 322.15 ms
(s the same as training ImageNet with use of vertical flip.)59.5 309.15 ms
(Training: At the pretraining stage we used one cycle of cosine a)59.5 283.15 ms
(nnealing with warm up. The maximum lr is 0.001, and the number o)59.5 270.15 ms
(f epochs is ranged between 50 and 70 for different setups. At th)59.5 257.15 ms
(e stage of fine tuning we applied ReduceLROnPlateau several time)59.5 244.15 ms
(s to alternate high and low lr. The code is implemented with Pyt)59.5 231.15 ms
(orch. The total time of training for one fold is 1-2 hours, so t)59.5 218.15 ms
(he training of entire model is 6-9 hours. Almost all our models )59.5 205.15 ms
(were trained at kaggle, and the kernel is at this link.)59.5 192.15 ms
(Submission:)59.5 166.15 ms
(Use of 64 mels has allowed us also to shorten the inference time)59.5 153.15 ms
(. In particular, each 5-fold model takes only 1-1.5 min for gene)59.5 140.15 ms
(ration of a prediction with 8TTA. Therefore, we were able to use)59.5 127.15 ms
( an ensemble of 15 models to generate the final submissions.)59.5 114.15 ms
(The final remark about the possible reason of the gap between CV)59.5 101.15 ms
( and public LB: this paper \(Task 1B\) reports 0.15 lwlrap drop fo)59.5 88.15 ms
(r test performed on data recorded with different device vs the s)59.5 75.15 ms
(ame device as used for training data. So, the public LB data may)59.5 62.15 ms
( be selected for a different device, but we never know if it is )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 4 4
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(smaller_faster.txt                                        Page 4)59.5 790.15 ms
F0 sf
(true or not.)59.5 764.15 ms
(===)59.5 751.15 ms
(Thanks for sharing the method!!)59.5 738.15 ms
(Always learned a lot from you , @iafoss .)59.5 725.15 ms
(Nice new paper implement ! \(Spectral augmentation\))59.5 712.15 ms
(Hope you get a good result for this competition !!)59.5 699.15 ms
(Options)59.5 686.15 ms
(You are welcome and thanks, u2 best luck at the stage 2)59.5 673.15 ms
(May I ask, have your team used any kind of minority class oversa)59.5 647.15 ms
(mpling? Have you applied MixUp with some probability or just aug)59.5 634.15 ms
(mented the whole dataset? Thanks for sharing and good luck in fi)59.5 621.15 ms
(nal shakeup!)59.5 608.15 ms
(Thanks, and good luck at the stage 2.)59.5 595.15 ms
(We have applied it to all samples in a batch, and we did not use)59.5 582.15 ms
( any special consideration for any classes. The implementation i)59.5 569.15 ms
(s borrowed from here.)59.5 556.15 ms
(Thanks for sharing!)59.5 543.15 ms
(I should have tried 64 mels, it looks promising in your writing )59.5 530.15 ms
(;\))59.5 517.15 ms
(@ebouteillon, I wish u also the best luck. The idea suggested by)59.5 491.15 ms
( @theoviel about 64 mels is really unexpected to work. However, )59.5 478.15 ms
(when I tried to switch back to 128 mels in our best setup later,)59.5 465.15 ms
( I didn't see improvement. The thing could be also that for 128 )59.5 452.15 ms
(mels ~100 epochs \(in total\) was not enough since stage1 Dnet mod)59.5 439.15 ms
(els we trained for 300-400 epochs.)59.5 426.15 ms
(Thanks so much @iafoss and all your team for the very detailed s)59.5 413.15 ms
(olution. I did try 4s \(but with 128 mel\) and had no improvement,)59.5 400.15 ms
( perhaps I have to go back and redo my experiment :\))59.5 387.15 ms
(Very nice writeup @iafoss)59.5 361.15 ms
(I miss your awesome kernels.. There were great kernels in this c)59.5 348.15 ms
(ompetition especially those of @daisukelab and @mhiro2)59.5 335.15 ms
(I had almost the same experiments like yours \(only part of what )59.5 322.15 ms
(you did\).. I wished I had more time than the last 2 days for thi)59.5 309.15 ms
(s comp and those 2 days was hopeless with only 4 submissions , w)59.5 296.15 ms
(hich was surprising that with a good dose of luck it got into th)59.5 283.15 ms
(e silver range.)59.5 270.15 ms
(One question about your model arch. How did you think about chan)59.5 257.15 ms
(ging the arch into what you did? Is it try and see, or you had a)59.5 244.15 ms
(n idea that the model should look like this and not that?)59.5 231.15 ms
(I wish you and your team all the best in the 2nd stage. I have l)59.5 218.15 ms
(earned a lot from your kernels and from @suicaokhoailang and @ra)59.5 205.15 ms
(dek1 sharing in the past competitions.)59.5 192.15 ms
(@hwasiti, It is quite impressive that u could get that far just )59.5 179.15 ms
(within 2 days, hope u also get a good score at the stage 2. Rega)59.5 166.15 ms
(rding your question, the main reason why I focuses mostly on den)59.5 153.15 ms
(se connections \(within the conv block and pyramid pooling\) is th)59.5 140.15 ms
(at Dnet models worked much better than ResNet, ResNeXt, CBAM Res)59.5 127.15 ms
(NeXt, NasNet, etc. at stage 1. Also, concat pooling gave quite n)59.5 114.15 ms
(oticeable boost in my early testing. Later I tried other things )59.5 101.15 ms
(too, but they didn't work that well. The thing I didn't expect i)59.5 88.15 ms
(s that traditional 7x7 conv followed by pooling in the first lay)59.5 75.15 ms
(er doesn't really work here. Probably, if one just took Dnet121 )59.5 62.15 ms
(and replace its first block, it could work quite well on 64 mels)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 5 5
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(smaller_faster.txt                                        Page 5)59.5 790.15 ms
F0 sf
(, but I didn't have time to check it. And thank you for your bes)59.5 764.15 ms
(t wishes.)59.5 751.15 ms
(Thanks for sharing, I didn't even imagine decreasing mels down t)59.5 738.15 ms
(o 64.)59.5 725.15 ms
(And I agree with mixup is so efficient in audio task, "sounds ar)59.5 712.15 ms
(e transparent" - that's it.)59.5 699.15 ms
(I hope your team's good luck in 2nd stage.)59.5 686.15 ms
(Thanks, u2. And thank you for your kernels, I used one as a star)59.5 660.15 ms
(ting point for mel preprocessing.)59.5 647.15 ms
(@iafoss Thanks for sharing in detail. Maybe I should try to redu)59.5 621.15 ms
(ce mel to 64 and increase duration to 4s. Haven't thought about )59.5 608.15 ms
(reducing mel would gain better CV or LB)59.5 595.15 ms
(The key point with 64 mel is using small models with optimized a)59.5 569.15 ms
(rchitecture. Regular computer vision models require 128 mels, an)59.5 556.15 ms
(d also may be up-scaling as we did in our first attempts, that m)59.5 543.15 ms
(akes the models quite slow at training and inference. Meanwhile )59.5 530.15 ms
(with 64 mel setup we could reach 0.715 public LB score for a sin)59.5 517.15 ms
(gle 5-fold model trained within one kernel \(~6 hours\). Ensemblin)59.5 504.15 ms
(g boosted it to 0.739.)59.5 491.15 ms
(We will release the kernel used for training the models when pri)59.5 465.15 ms
(vate LB is available.)59.5 452.15 ms
(Great, really keen to see your kernel.)59.5 426.15 ms
(The kernel right now is available at https://www.kaggle.com/theo)59.5 400.15 ms
(viel/9th-place-modeling-kernel)59.5 387.15 ms
(@iafoss Great work. And congrats to be Kaggle Competition Master)59.5 361.15 ms
(. Every time I can learn a lot from your kernel. Also learned a )59.5 348.15 ms
(lot from your Airbus kernels.)59.5 335.15 ms
(Thanks so much, I'm happy to know that my kernels were useful.)59.5 309.15 ms
re sp
%%PageTrailer
%%Trailer
%%Pages: 5
%%EOF
